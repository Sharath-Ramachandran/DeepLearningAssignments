{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations :\n",
    "The top 3 scores obtained were :\n",
    "1. 0.14366\n",
    "2. 0.14945\n",
    "3. 0.15538\n",
    "All the scores have relu  activation with Adam Optimizer and MSE loss.\n",
    "\n",
    "The first score was obtained by using 210 epochs, learning rate=0.01 and 2100 neurtons in the hidden layer\n",
    "The second score was obtained by using 200 epochs, learning rate =0.001 and 2000 neurons in the hidden layer\n",
    "The third score was obtained by 150 epochs, learning rate 0.001 and 2000 neurons in the hidden layer.\n",
    "\n",
    "As the number of neurons and epochs increased, the network has been trained well and hence the score has increased.Also, with the top score architecture, when I added a batch size of 128 to the data, I got a reduced score. Noticed that the normalization reduced the score , so didnt perform normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](kaggle_result.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModel(dataLoader):\n",
    "    for epoch in range(210):\n",
    "        ave_loss=0.0\n",
    "        print(\"Epoch \",epoch+1)\n",
    "        for i,data in enumerate(dataLoader,0):\n",
    "            inputs,target =data\n",
    "            target = target.float()\n",
    "            optimizer.zero_grad()\n",
    "            output=model(inputs)\n",
    "            loss=criteria(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ave_loss+=loss.item()\n",
    "        print(\"Loss for Epoch \",epoch+1,\" is \",(ave_loss/x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreProcessing(dataframe):\n",
    "    print(\"Number of NAN columns before preprocessing\\n\",dataframe.isna().sum().sort_values()[-30:-1]) # prints the top 35 nan valued columns\n",
    "    \n",
    "    #Filling with \"NA\" string\n",
    "    for col in ['Alley','FireplaceQu','Fence','MiscFeature','PoolQC','GarageQual','GarageFinish','GarageType','GarageCond','BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure']:\n",
    "        dataframe[col].fillna('NA', inplace=True)\n",
    "    #Fill numerical columns with  mode value\n",
    "    fill_avg = ['KitchenQual','Exterior1st','SaleType','Exterior2nd','Functional','Utilities','MSZoning','LotFrontage','Electrical','MasVnrArea','GarageYrBlt']\n",
    "    for i in fill_avg:\n",
    "        dataframe[i].fillna(dataframe[i].value_counts().to_frame().index[0], inplace=True)\n",
    "    \n",
    "    median_values_col=['GarageArea','GarageCars','BsmtFinSF1','TotalBsmtSF','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','BsmtUnfSF']\n",
    "    for i in median_values_col: # did it separately as the test data had nan in it.. didnt find it at training dataframe\n",
    "        dataframe[i].fillna(dataframe[i].value_counts().to_frame().index[0], inplace=True)\n",
    "        \n",
    "    print(\"Number of NAN columns after preprocessing\\n\",dataframe.isna().sum().sort_values()[-10:-1])\n",
    "    \n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding of the categorical columns in the dataframe\n",
    "def performOnehotEncoding(dataframe):\n",
    "    col_groups = dataframe.columns.to_series().groupby(dataframe.dtypes).groups\n",
    "    non_numeric_cols = col_groups[np.dtype('O')]\n",
    "    for col in non_numeric_cols:\n",
    "        one_hot = pd.get_dummies(dataframe[col],prefix=str(col+\"_\"))\n",
    "        dataframe = dataframe.drop(col,axis=1)\n",
    "        dataframe = dataframe.join(one_hot)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets the columns not in test but present in train and adds in the\n",
    "# index present in the train dataframe\n",
    "def addColumnsFromTraintoTest(train,test):\n",
    "    for i in train.columns:\n",
    "        if i not in test.columns:\n",
    "            index= train.columns.get_loc(i)\n",
    "            test.insert(index,i,0)\n",
    "    return test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the dataframe to numpy array to tensor array\n",
    "def changeToTensor(dataFrame):\n",
    "    x = dataFrame.values\n",
    "    x= x.astype('float64')\n",
    "    x= torch.Tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"house-prices-advanced-regression-techniques/train.csv\")\n",
    "test = pd.read_csv(\"house-prices-advanced-regression-techniques/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAN columns before preprocessing\n",
      " YearBuilt          0\n",
      "RoofStyle          0\n",
      "RoofMatl           0\n",
      "Exterior1st        0\n",
      "Exterior2nd        0\n",
      "ExterQual          0\n",
      "ExterCond          0\n",
      "Foundation         0\n",
      "MSSubClass         0\n",
      "YearRemodAdd       0\n",
      "Utilities          0\n",
      "Electrical         1\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "BsmtExposure      38\n",
      "GarageQual        81\n",
      "GarageFinish      81\n",
      "GarageYrBlt       81\n",
      "GarageType        81\n",
      "GarageCond        81\n",
      "LotFrontage      259\n",
      "FireplaceQu      690\n",
      "Fence           1179\n",
      "Alley           1369\n",
      "MiscFeature     1406\n",
      "dtype: int64\n",
      "Number of NAN columns after preprocessing\n",
      " ExterCond      0\n",
      "ExterQual      0\n",
      "MasVnrArea     0\n",
      "Exterior2nd    0\n",
      "Exterior1st    0\n",
      "RoofMatl       0\n",
      "RoofStyle      0\n",
      "Heating        0\n",
      "SalePrice      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = dataPreProcessing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAN columns before preprocessing\n",
      " Exterior1st        1\n",
      "SaleType           1\n",
      "TotalBsmtSF        1\n",
      "BsmtUnfSF          1\n",
      "Exterior2nd        1\n",
      "BsmtFinSF1         1\n",
      "BsmtFinSF2         1\n",
      "BsmtFullBath       2\n",
      "Functional         2\n",
      "Utilities          2\n",
      "BsmtHalfBath       2\n",
      "MSZoning           4\n",
      "MasVnrArea        15\n",
      "MasVnrType        16\n",
      "BsmtFinType2      42\n",
      "BsmtFinType1      42\n",
      "BsmtExposure      44\n",
      "BsmtQual          44\n",
      "BsmtCond          45\n",
      "GarageType        76\n",
      "GarageFinish      78\n",
      "GarageYrBlt       78\n",
      "GarageQual        78\n",
      "GarageCond        78\n",
      "LotFrontage      227\n",
      "FireplaceQu      730\n",
      "Fence           1169\n",
      "Alley           1352\n",
      "MiscFeature     1408\n",
      "dtype: int64\n",
      "Number of NAN columns after preprocessing\n",
      " ExterCond        0\n",
      "ExterQual        0\n",
      "MasVnrArea       0\n",
      "Exterior2nd      0\n",
      "Exterior1st      0\n",
      "RoofMatl         0\n",
      "RoofStyle        0\n",
      "SaleType         0\n",
      "SaleCondition    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test = dataPreProcessing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave    NA      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave    NA      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave    NA      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave    NA      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave    NA      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0     NA    NA          NA       0      2   \n",
       "1         Lvl    AllPub  ...        0     NA    NA          NA       0      5   \n",
       "2         Lvl    AllPub  ...        0     NA    NA          NA       0      9   \n",
       "3         Lvl    AllPub  ...        0     NA    NA          NA       0      2   \n",
       "4         Lvl    AllPub  ...        0     NA    NA          NA       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NA</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0  1461          20       RH         80.0    11622   Pave    NA      Reg   \n",
       "1  1462          20       RL         81.0    14267   Pave    NA      IR1   \n",
       "2  1463          60       RL         74.0    13830   Pave    NA      IR1   \n",
       "3  1464          60       RL         78.0     9978   Pave    NA      IR1   \n",
       "4  1465         120       RL         43.0     5005   Pave    NA      IR1   \n",
       "\n",
       "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature  \\\n",
       "0         Lvl    AllPub  ...         120        0     NA  MnPrv          NA   \n",
       "1         Lvl    AllPub  ...           0        0     NA     NA        Gar2   \n",
       "2         Lvl    AllPub  ...           0        0     NA  MnPrv          NA   \n",
       "3         Lvl    AllPub  ...           0        0     NA     NA          NA   \n",
       "4         HLS    AllPub  ...         144        0     NA     NA          NA   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0       0      6    2010        WD         Normal  \n",
       "1   12500      6    2010        WD         Normal  \n",
       "2       0      3    2010        WD         Normal  \n",
       "3       0      6    2010        WD         Normal  \n",
       "4       0      1    2010        WD         Normal  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num= train.values\n",
    "test_num= test.values\n",
    "labels=train_num[:,-1]\n",
    "train.drop(['SalePrice','Id'],axis=1, inplace=True)\n",
    "test.drop('Id',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = performOnehotEncoding(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = performOnehotEncoding(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = addColumnsFromTraintoTest(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = changeToTensor(train)\n",
    "labels= labels.astype('float64')\n",
    "labels= torch.Tensor(labels)\n",
    "x_test = changeToTensor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(size,2100)\n",
    "        self.fc6 = nn.Linear(2100,1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1460, 302]), torch.Size([1460]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 302])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= utils.TensorDataset(x_train,labels)\n",
    "dataLoader = utils.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=302, out_features=2100, bias=True)\n",
       "  (fc6): Linear(in_features=2100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria= nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/test/venv/lib/python3.5/site-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for Epoch  1  is  4701269376.414348\n",
      "Epoch  2\n",
      "Loss for Epoch  2  is  2900069181.1582117\n",
      "Epoch  3\n",
      "Loss for Epoch  3  is  2451435120.5033584\n",
      "Epoch  4\n",
      "Loss for Epoch  4  is  2240945621.929738\n",
      "Epoch  5\n",
      "Loss for Epoch  5  is  2142103883.2658536\n",
      "Epoch  6\n",
      "Loss for Epoch  6  is  2385008599.4340467\n",
      "Epoch  7\n",
      "Loss for Epoch  7  is  1982733974.6796942\n",
      "Epoch  8\n",
      "Loss for Epoch  8  is  1947689995.3613453\n",
      "Epoch  9\n",
      "Loss for Epoch  9  is  1882199822.4821603\n",
      "Epoch  10\n",
      "Loss for Epoch  10  is  1852823603.5840547\n",
      "Epoch  11\n",
      "Loss for Epoch  11  is  1805629136.2024505\n",
      "Epoch  12\n",
      "Loss for Epoch  12  is  1727306753.4164557\n",
      "Epoch  13\n",
      "Loss for Epoch  13  is  1696984887.7612686\n",
      "Epoch  14\n",
      "Loss for Epoch  14  is  1610500092.956284\n",
      "Epoch  15\n",
      "Loss for Epoch  15  is  1557749979.3521786\n",
      "Epoch  16\n",
      "Loss for Epoch  16  is  1620064890.0085974\n",
      "Epoch  17\n",
      "Loss for Epoch  17  is  1501793979.1791956\n",
      "Epoch  18\n",
      "Loss for Epoch  18  is  1459644136.5274823\n",
      "Epoch  19\n",
      "Loss for Epoch  19  is  1425743313.510751\n",
      "Epoch  20\n",
      "Loss for Epoch  20  is  1475455414.485823\n",
      "Epoch  21\n",
      "Loss for Epoch  21  is  1386765544.768356\n",
      "Epoch  22\n",
      "Loss for Epoch  22  is  1374579846.0511234\n",
      "Epoch  23\n",
      "Loss for Epoch  23  is  1354518640.3604872\n",
      "Epoch  24\n",
      "Loss for Epoch  24  is  1570770059.8783035\n",
      "Epoch  25\n",
      "Loss for Epoch  25  is  1951666249.604674\n",
      "Epoch  26\n",
      "Loss for Epoch  26  is  1300983864.586891\n",
      "Epoch  27\n",
      "Loss for Epoch  27  is  1344479519.5133715\n",
      "Epoch  28\n",
      "Loss for Epoch  28  is  1696976269.2671835\n",
      "Epoch  29\n",
      "Loss for Epoch  29  is  1323608607.092001\n",
      "Epoch  30\n",
      "Loss for Epoch  30  is  1251686621.2175066\n",
      "Epoch  31\n",
      "Loss for Epoch  31  is  1292729410.641195\n",
      "Epoch  32\n",
      "Loss for Epoch  32  is  1249662421.5333583\n",
      "Epoch  33\n",
      "Loss for Epoch  33  is  1254312527.458064\n",
      "Epoch  34\n",
      "Loss for Epoch  34  is  1264690085.8094568\n",
      "Epoch  35\n",
      "Loss for Epoch  35  is  1226913826.6318746\n",
      "Epoch  36\n",
      "Loss for Epoch  36  is  1227675861.6452336\n",
      "Epoch  37\n",
      "Loss for Epoch  37  is  1320975823.2777667\n",
      "Epoch  38\n",
      "Loss for Epoch  38  is  1120996019.187985\n",
      "Epoch  39\n",
      "Loss for Epoch  39  is  1379609621.337674\n",
      "Epoch  40\n",
      "Loss for Epoch  40  is  1182088502.1994932\n",
      "Epoch  41\n",
      "Loss for Epoch  41  is  1301613623.945919\n",
      "Epoch  42\n",
      "Loss for Epoch  42  is  1154501754.9711225\n",
      "Epoch  43\n",
      "Loss for Epoch  43  is  1295265388.105829\n",
      "Epoch  44\n",
      "Loss for Epoch  44  is  1107077438.3322668\n",
      "Epoch  45\n",
      "Loss for Epoch  45  is  1269331487.3594284\n",
      "Epoch  46\n",
      "Loss for Epoch  46  is  1108871837.3883226\n",
      "Epoch  47\n",
      "Loss for Epoch  47  is  1254098263.4124916\n",
      "Epoch  48\n",
      "Loss for Epoch  48  is  1096052168.9603481\n",
      "Epoch  49\n",
      "Loss for Epoch  49  is  1210477123.3118677\n",
      "Epoch  50\n",
      "Loss for Epoch  50  is  1036485463.3726876\n",
      "Epoch  51\n",
      "Loss for Epoch  51  is  1218977598.14474\n",
      "Epoch  52\n",
      "Loss for Epoch  52  is  1047462710.6556281\n",
      "Epoch  53\n",
      "Loss for Epoch  53  is  1215362840.6651423\n",
      "Epoch  54\n",
      "Loss for Epoch  54  is  1019929985.2323763\n",
      "Epoch  55\n",
      "Loss for Epoch  55  is  1167054679.001913\n",
      "Epoch  56\n",
      "Loss for Epoch  56  is  1047027000.8993714\n",
      "Epoch  57\n",
      "Loss for Epoch  57  is  1116990068.5506823\n",
      "Epoch  58\n",
      "Loss for Epoch  58  is  969304839.7316098\n",
      "Epoch  59\n",
      "Loss for Epoch  59  is  1177116457.6722074\n",
      "Epoch  60\n",
      "Loss for Epoch  60  is  966568384.8560097\n",
      "Epoch  61\n",
      "Loss for Epoch  61  is  1155130715.749252\n",
      "Epoch  62\n",
      "Loss for Epoch  62  is  969916059.8721536\n",
      "Epoch  63\n",
      "Loss for Epoch  63  is  1088262202.7495937\n",
      "Epoch  64\n",
      "Loss for Epoch  64  is  943403028.9574274\n",
      "Epoch  65\n",
      "Loss for Epoch  65  is  1103385251.4729314\n",
      "Epoch  66\n",
      "Loss for Epoch  66  is  949870676.2526128\n",
      "Epoch  67\n",
      "Loss for Epoch  67  is  1083053742.1419091\n",
      "Epoch  68\n",
      "Loss for Epoch  68  is  922298062.7366645\n",
      "Epoch  69\n",
      "Loss for Epoch  69  is  1073468904.0984418\n",
      "Epoch  70\n",
      "Loss for Epoch  70  is  936496890.4674613\n",
      "Epoch  71\n",
      "Loss for Epoch  71  is  1084141616.9414363\n",
      "Epoch  72\n",
      "Loss for Epoch  72  is  908057251.2398685\n",
      "Epoch  73\n",
      "Loss for Epoch  73  is  1036718841.4358671\n",
      "Epoch  74\n",
      "Loss for Epoch  74  is  866926093.8608983\n",
      "Epoch  75\n",
      "Loss for Epoch  75  is  1020273213.3216025\n",
      "Epoch  76\n",
      "Loss for Epoch  76  is  892556571.2508698\n",
      "Epoch  77\n",
      "Loss for Epoch  77  is  1022565736.0497881\n",
      "Epoch  78\n",
      "Loss for Epoch  78  is  866290849.1170604\n",
      "Epoch  79\n",
      "Loss for Epoch  79  is  1027845437.4964877\n",
      "Epoch  80\n",
      "Loss for Epoch  80  is  874387187.8072468\n",
      "Epoch  81\n",
      "Loss for Epoch  81  is  989087929.2084651\n",
      "Epoch  82\n",
      "Loss for Epoch  82  is  851616119.1781155\n",
      "Epoch  83\n",
      "Loss for Epoch  83  is  990589845.4039335\n",
      "Epoch  84\n",
      "Loss for Epoch  84  is  860571139.0646424\n",
      "Epoch  85\n",
      "Loss for Epoch  85  is  951102007.3615875\n",
      "Epoch  86\n",
      "Loss for Epoch  86  is  820332364.0833\n",
      "Epoch  87\n",
      "Loss for Epoch  87  is  994247429.9416314\n",
      "Epoch  88\n",
      "Loss for Epoch  88  is  824724062.2405882\n",
      "Epoch  89\n",
      "Loss for Epoch  89  is  951375560.5582453\n",
      "Epoch  90\n",
      "Loss for Epoch  90  is  826963538.5121397\n",
      "Epoch  91\n",
      "Loss for Epoch  91  is  912393717.5669874\n",
      "Epoch  92\n",
      "Loss for Epoch  92  is  825893024.5886207\n",
      "Epoch  93\n",
      "Loss for Epoch  93  is  943713769.0192754\n",
      "Epoch  94\n",
      "Loss for Epoch  94  is  795093378.982127\n",
      "Epoch  95\n",
      "Loss for Epoch  95  is  933097396.7895249\n",
      "Epoch  96\n",
      "Loss for Epoch  96  is  786954127.6874229\n",
      "Epoch  97\n",
      "Loss for Epoch  97  is  923548295.6775601\n",
      "Epoch  98\n",
      "Loss for Epoch  98  is  796032149.0671557\n",
      "Epoch  99\n",
      "Loss for Epoch  99  is  922907346.366336\n",
      "Epoch  100\n",
      "Loss for Epoch  100  is  775258921.3461983\n",
      "Epoch  101\n",
      "Loss for Epoch  101  is  868868611.6881505\n",
      "Epoch  102\n",
      "Loss for Epoch  102  is  809655290.9498147\n",
      "Epoch  103\n",
      "Loss for Epoch  103  is  903863049.3851309\n",
      "Epoch  104\n",
      "Loss for Epoch  104  is  778339400.6639467\n",
      "Epoch  105\n",
      "Loss for Epoch  105  is  887057793.5915797\n",
      "Epoch  106\n",
      "Loss for Epoch  106  is  776585664.4655312\n",
      "Epoch  107\n",
      "Loss for Epoch  107  is  893325750.8449051\n",
      "Epoch  108\n",
      "Loss for Epoch  108  is  754927252.9742771\n",
      "Epoch  109\n",
      "Loss for Epoch  109  is  887137385.4216671\n",
      "Epoch  110\n",
      "Loss for Epoch  110  is  764772341.503821\n",
      "Epoch  111\n",
      "Loss for Epoch  111  is  868770938.5726795\n",
      "Epoch  112\n",
      "Loss for Epoch  112  is  751064394.3984278\n",
      "Epoch  113\n",
      "Loss for Epoch  113  is  855755421.8410357\n",
      "Epoch  114\n",
      "Loss for Epoch  114  is  780682212.084745\n",
      "Epoch  115\n",
      "Loss for Epoch  115  is  787874994.667732\n",
      "Epoch  116\n",
      "Loss for Epoch  116  is  742492213.6371537\n",
      "Epoch  117\n",
      "Loss for Epoch  117  is  806937697.3064498\n",
      "Epoch  118\n",
      "Loss for Epoch  118  is  720548690.8008933\n",
      "Epoch  119\n",
      "Loss for Epoch  119  is  885357209.8657088\n",
      "Epoch  120\n",
      "Loss for Epoch  120  is  701810656.4579262\n",
      "Epoch  121\n",
      "Loss for Epoch  121  is  802627848.3984679\n",
      "Epoch  122\n",
      "Loss for Epoch  122  is  725063245.4600215\n",
      "Epoch  123\n",
      "Loss for Epoch  123  is  751388174.9949394\n",
      "Epoch  124\n",
      "Loss for Epoch  124  is  715620488.8727942\n",
      "Epoch  125\n",
      "Loss for Epoch  125  is  736866851.4099922\n",
      "Epoch  126\n",
      "Loss for Epoch  126  is  674912184.5356368\n",
      "Epoch  127\n",
      "Loss for Epoch  127  is  682439928.627945\n",
      "Epoch  128\n",
      "Loss for Epoch  128  is  883734335.1642083\n",
      "Epoch  129\n",
      "Loss for Epoch  129  is  632143113.2220604\n",
      "Epoch  130\n",
      "Loss for Epoch  130  is  710830909.2136867\n",
      "Epoch  131\n",
      "Loss for Epoch  131  is  680664013.860488\n",
      "Epoch  132\n",
      "Loss for Epoch  132  is  741688804.4175327\n",
      "Epoch  133\n",
      "Loss for Epoch  133  is  654125986.7759538\n",
      "Epoch  134\n",
      "Loss for Epoch  134  is  761914680.1256295\n",
      "Epoch  135\n",
      "Loss for Epoch  135  is  670057119.6612512\n",
      "Epoch  136\n",
      "Loss for Epoch  136  is  689780994.6211879\n",
      "Epoch  137\n",
      "Loss for Epoch  137  is  675694991.3628743\n",
      "Epoch  138\n",
      "Loss for Epoch  138  is  699600042.3167768\n",
      "Epoch  139\n",
      "Loss for Epoch  139  is  637620957.811536\n",
      "Epoch  140\n",
      "Loss for Epoch  140  is  673251544.941043\n",
      "Epoch  141\n",
      "Loss for Epoch  141  is  692675576.9512628\n",
      "Epoch  142\n",
      "Loss for Epoch  142  is  641944024.3525287\n",
      "Epoch  143\n",
      "Loss for Epoch  143  is  628182418.2456053\n",
      "Epoch  144\n",
      "Loss for Epoch  144  is  732774146.5406333\n",
      "Epoch  145\n",
      "Loss for Epoch  145  is  660995622.9410405\n",
      "Epoch  146\n",
      "Loss for Epoch  146  is  682739988.9764969\n",
      "Epoch  147\n",
      "Loss for Epoch  147  is  658103047.055062\n",
      "Epoch  148\n",
      "Loss for Epoch  148  is  707268869.0468367\n",
      "Epoch  149\n",
      "Loss for Epoch  149  is  626818844.8691713\n",
      "Epoch  150\n",
      "Loss for Epoch  150  is  634137173.5044235\n",
      "Epoch  151\n",
      "Loss for Epoch  151  is  746138614.4279311\n",
      "Epoch  152\n",
      "Loss for Epoch  152  is  614813537.4131917\n",
      "Epoch  153\n",
      "Loss for Epoch  153  is  714450713.173314\n",
      "Epoch  154\n",
      "Loss for Epoch  154  is  587956210.2070409\n",
      "Epoch  155\n",
      "Loss for Epoch  155  is  695953317.8556399\n",
      "Epoch  156\n",
      "Loss for Epoch  156  is  635781596.2541479\n",
      "Epoch  157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for Epoch  157  is  646451250.8616719\n",
      "Epoch  158\n",
      "Loss for Epoch  158  is  601105027.6529005\n",
      "Epoch  159\n",
      "Loss for Epoch  159  is  648224697.4252856\n",
      "Epoch  160\n",
      "Loss for Epoch  160  is  620955230.3389566\n",
      "Epoch  161\n",
      "Loss for Epoch  161  is  627975658.4404733\n",
      "Epoch  162\n",
      "Loss for Epoch  162  is  666441997.7695056\n",
      "Epoch  163\n",
      "Loss for Epoch  163  is  594916058.4071345\n",
      "Epoch  164\n",
      "Loss for Epoch  164  is  624623725.847572\n",
      "Epoch  165\n",
      "Loss for Epoch  165  is  642603986.8416963\n",
      "Epoch  166\n",
      "Loss for Epoch  166  is  598387249.8665967\n",
      "Epoch  167\n",
      "Loss for Epoch  167  is  553002409.8782372\n",
      "Epoch  168\n",
      "Loss for Epoch  168  is  622728767.2816402\n",
      "Epoch  169\n",
      "Loss for Epoch  169  is  714890629.8582613\n",
      "Epoch  170\n",
      "Loss for Epoch  170  is  592683339.2741858\n",
      "Epoch  171\n",
      "Loss for Epoch  171  is  580109805.5751145\n",
      "Epoch  172\n",
      "Loss for Epoch  172  is  653584884.9587489\n",
      "Epoch  173\n",
      "Loss for Epoch  173  is  564652965.9958824\n",
      "Epoch  174\n",
      "Loss for Epoch  174  is  605403285.5218008\n",
      "Epoch  175\n",
      "Loss for Epoch  175  is  594258263.7100792\n",
      "Epoch  176\n",
      "Loss for Epoch  176  is  589269783.780161\n",
      "Epoch  177\n",
      "Loss for Epoch  177  is  604534127.0539166\n",
      "Epoch  178\n",
      "Loss for Epoch  178  is  571877740.8279561\n",
      "Epoch  179\n",
      "Loss for Epoch  179  is  576159042.4121305\n",
      "Epoch  180\n",
      "Loss for Epoch  180  is  557563298.0366049\n",
      "Epoch  181\n",
      "Loss for Epoch  181  is  608674107.4021556\n",
      "Epoch  182\n",
      "Loss for Epoch  182  is  590618443.995691\n",
      "Epoch  183\n",
      "Loss for Epoch  183  is  674863643.8450842\n",
      "Epoch  184\n",
      "Loss for Epoch  184  is  536029983.9045009\n",
      "Epoch  185\n",
      "Loss for Epoch  185  is  581171674.5269027\n",
      "Epoch  186\n",
      "Loss for Epoch  186  is  576368096.7904443\n",
      "Epoch  187\n",
      "Loss for Epoch  187  is  549496939.1757951\n",
      "Epoch  188\n",
      "Loss for Epoch  188  is  601613592.3372645\n",
      "Epoch  189\n",
      "Loss for Epoch  189  is  724790061.1009473\n",
      "Epoch  190\n",
      "Loss for Epoch  190  is  530920115.85597473\n",
      "Epoch  191\n",
      "Loss for Epoch  191  is  504220547.04072887\n",
      "Epoch  192\n",
      "Loss for Epoch  192  is  583947403.6354649\n",
      "Epoch  193\n",
      "Loss for Epoch  193  is  551768736.1879236\n",
      "Epoch  194\n",
      "Loss for Epoch  194  is  554573638.3600472\n",
      "Epoch  195\n",
      "Loss for Epoch  195  is  547799727.6886833\n",
      "Epoch  196\n",
      "Loss for Epoch  196  is  539626047.8903642\n",
      "Epoch  197\n",
      "Loss for Epoch  197  is  755169985.4064465\n",
      "Epoch  198\n",
      "Loss for Epoch  198  is  545270798.807953\n",
      "Epoch  199\n",
      "Loss for Epoch  199  is  619652198.1888843\n",
      "Epoch  200\n",
      "Loss for Epoch  200  is  600402267.4722165\n",
      "Epoch  201\n",
      "Loss for Epoch  201  is  514773829.4740562\n",
      "Epoch  202\n",
      "Loss for Epoch  202  is  579022800.7655382\n",
      "Epoch  203\n",
      "Loss for Epoch  203  is  519757562.9957031\n",
      "Epoch  204\n",
      "Loss for Epoch  204  is  570167138.6350842\n",
      "Epoch  205\n",
      "Loss for Epoch  205  is  513305099.30770785\n",
      "Epoch  206\n",
      "Loss for Epoch  206  is  552426883.0319861\n",
      "Epoch  207\n",
      "Loss for Epoch  207  is  540213709.8435657\n",
      "Epoch  208\n",
      "Loss for Epoch  208  is  547231038.7703646\n",
      "Epoch  209\n",
      "Loss for Epoch  209  is  541361359.9558403\n",
      "Epoch  210\n",
      "Loss for Epoch  210  is  587257451.9414719\n"
     ]
    }
   ],
   "source": [
    "trainingModel(dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_array = predictions.data.cpu().numpy() # convert the tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the \n",
    "test_data = pd.read_csv(\"house-prices-advanced-regression-techniques/test.csv\")\n",
    "test_df=pd.DataFrame(columns=['Id','SalePrice'])\n",
    "test_df[\"Id\"]= test_data[\"Id\"]\n",
    "test_df[\"SalePrice\"]= predictions_array\n",
    "test_df[[\"Id\",\"SalePrice\"]].to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
