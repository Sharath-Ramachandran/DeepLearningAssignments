{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the dataframe to numpy array to tensor array\n",
    "def changeToTensor(dataFrame):\n",
    "    dataFrame= (dataFrame-dataFrame.min())/(dataFrame.max()-dataFrame.min())\n",
    "    x = dataFrame.values\n",
    "    x= x.astype('float64')\n",
    "    x= torch.Tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.txt',sep=',')\n",
    "test_data = pd.read_csv('test.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.00</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>426.0</td>\n",
       "      <td>713.50</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>704.50</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
       "1  2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
       "2  2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
       "3  2015-02-04 17:53:00        23.15   27.2450  426.0  713.50       0.004779   \n",
       "4  2015-02-04 17:54:00        23.15   27.2000  426.0  708.25       0.004772   \n",
       "5  2015-02-04 17:55:00        23.10   27.2000  426.0  704.50       0.004757   \n",
       "\n",
       "   Occupancy  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a model with pytorch#####\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(size,1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numpy = train_data.values\n",
    "labels = train_numpy[:,-1]\n",
    "labels= labels.astype('float64')\n",
    "labels= torch.Tensor(labels)\n",
    "train_sub = train_data.drop(['date','Occupancy'],axis=1)\n",
    "x_train = changeToTensor(train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8143, 5]), torch.Size([8143, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=5, out_features=1000, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=1000, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria= nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, batch_size=50):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for beg_i in range(0, x_train.size(0), batch_size):\n",
    "        x_batch = x_train[beg_i:beg_i + batch_size, :]\n",
    "        y_batch = labels[beg_i:beg_i + batch_size, :]\n",
    "        opt.zero_grad()\n",
    "        y_hat = model(x_batch)\n",
    "        loss = criterion(y_hat, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()        \n",
    "        losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch  1 loss is  [array(0.6544039, dtype=float32)]\n",
      "For epoch  2 loss is  [array(0.6208042, dtype=float32)]\n",
      "For epoch  3 loss is  [array(0.59221506, dtype=float32)]\n",
      "For epoch  4 loss is  [array(0.5683771, dtype=float32)]\n",
      "For epoch  5 loss is  [array(0.548845, dtype=float32)]\n",
      "For epoch  6 loss is  [array(0.5330599, dtype=float32)]\n",
      "For epoch  7 loss is  [array(0.52036905, dtype=float32)]\n",
      "For epoch  8 loss is  [array(0.5100731, dtype=float32)]\n",
      "For epoch  9 loss is  [array(0.5014838, dtype=float32)]\n",
      "For epoch  10 loss is  [array(0.49397224, dtype=float32)]\n",
      "For epoch  11 loss is  [array(0.48702097, dtype=float32)]\n",
      "For epoch  12 loss is  [array(0.48023036, dtype=float32)]\n",
      "For epoch  13 loss is  [array(0.47332785, dtype=float32)]\n",
      "For epoch  14 loss is  [array(0.466157, dtype=float32)]\n",
      "For epoch  15 loss is  [array(0.45864365, dtype=float32)]\n",
      "For epoch  16 loss is  [array(0.45078838, dtype=float32)]\n",
      "For epoch  17 loss is  [array(0.44263998, dtype=float32)]\n",
      "For epoch  18 loss is  [array(0.4342834, dtype=float32)]\n",
      "For epoch  19 loss is  [array(0.42581856, dtype=float32)]\n",
      "For epoch  20 loss is  [array(0.41735774, dtype=float32)]\n",
      "For epoch  21 loss is  [array(0.4090059, dtype=float32)]\n",
      "For epoch  22 loss is  [array(0.40085393, dtype=float32)]\n",
      "For epoch  23 loss is  [array(0.39297262, dtype=float32)]\n",
      "For epoch  24 loss is  [array(0.38541228, dtype=float32)]\n",
      "For epoch  25 loss is  [array(0.37818602, dtype=float32)]\n",
      "For epoch  26 loss is  [array(0.3712891, dtype=float32)]\n",
      "For epoch  27 loss is  [array(0.36468965, dtype=float32)]\n",
      "For epoch  28 loss is  [array(0.35834095, dtype=float32)]\n",
      "For epoch  29 loss is  [array(0.352185, dtype=float32)]\n",
      "For epoch  30 loss is  [array(0.34616214, dtype=float32)]\n",
      "For epoch  31 loss is  [array(0.34022292, dtype=float32)]\n",
      "For epoch  32 loss is  [array(0.3343291, dtype=float32)]\n",
      "For epoch  33 loss is  [array(0.32845947, dtype=float32)]\n",
      "For epoch  34 loss is  [array(0.32260564, dtype=float32)]\n",
      "For epoch  35 loss is  [array(0.31677786, dtype=float32)]\n",
      "For epoch  36 loss is  [array(0.31099626, dtype=float32)]\n",
      "For epoch  37 loss is  [array(0.30528829, dtype=float32)]\n",
      "For epoch  38 loss is  [array(0.29968092, dtype=float32)]\n",
      "For epoch  39 loss is  [array(0.2941997, dtype=float32)]\n",
      "For epoch  40 loss is  [array(0.2888633, dtype=float32)]\n",
      "For epoch  41 loss is  [array(0.2836818, dtype=float32)]\n",
      "For epoch  42 loss is  [array(0.27865794, dtype=float32)]\n",
      "For epoch  43 loss is  [array(0.27378806, dtype=float32)]\n",
      "For epoch  44 loss is  [array(0.2690627, dtype=float32)]\n",
      "For epoch  45 loss is  [array(0.26447037, dtype=float32)]\n",
      "For epoch  46 loss is  [array(0.26000088, dtype=float32)]\n",
      "For epoch  47 loss is  [array(0.25564143, dtype=float32)]\n",
      "For epoch  48 loss is  [array(0.251382, dtype=float32)]\n",
      "For epoch  49 loss is  [array(0.24721678, dtype=float32)]\n",
      "For epoch  50 loss is  [array(0.24314076, dtype=float32)]\n",
      "For epoch  51 loss is  [array(0.23915388, dtype=float32)]\n",
      "For epoch  52 loss is  [array(0.23525882, dtype=float32)]\n",
      "For epoch  53 loss is  [array(0.23145981, dtype=float32)]\n",
      "For epoch  54 loss is  [array(0.22776082, dtype=float32)]\n",
      "For epoch  55 loss is  [array(0.22416621, dtype=float32)]\n",
      "For epoch  56 loss is  [array(0.22067514, dtype=float32)]\n",
      "For epoch  57 loss is  [array(0.21728912, dtype=float32)]\n",
      "For epoch  58 loss is  [array(0.21400279, dtype=float32)]\n",
      "For epoch  59 loss is  [array(0.21081048, dtype=float32)]\n",
      "For epoch  60 loss is  [array(0.20770694, dtype=float32)]\n",
      "For epoch  61 loss is  [array(0.20468527, dtype=float32)]\n",
      "For epoch  62 loss is  [array(0.20174135, dtype=float32)]\n",
      "For epoch  63 loss is  [array(0.19887291, dtype=float32)]\n",
      "For epoch  64 loss is  [array(0.19607514, dtype=float32)]\n",
      "For epoch  65 loss is  [array(0.19334471, dtype=float32)]\n",
      "For epoch  66 loss is  [array(0.19067766, dtype=float32)]\n",
      "For epoch  67 loss is  [array(0.18807413, dtype=float32)]\n",
      "For epoch  68 loss is  [array(0.18553446, dtype=float32)]\n",
      "For epoch  69 loss is  [array(0.18305935, dtype=float32)]\n",
      "For epoch  70 loss is  [array(0.18064769, dtype=float32)]\n",
      "For epoch  71 loss is  [array(0.17829654, dtype=float32)]\n",
      "For epoch  72 loss is  [array(0.17600663, dtype=float32)]\n",
      "For epoch  73 loss is  [array(0.17377464, dtype=float32)]\n",
      "For epoch  74 loss is  [array(0.17159908, dtype=float32)]\n",
      "For epoch  75 loss is  [array(0.16947679, dtype=float32)]\n",
      "For epoch  76 loss is  [array(0.16740406, dtype=float32)]\n",
      "For epoch  77 loss is  [array(0.16538176, dtype=float32)]\n",
      "For epoch  78 loss is  [array(0.1634052, dtype=float32)]\n",
      "For epoch  79 loss is  [array(0.16147341, dtype=float32)]\n",
      "For epoch  80 loss is  [array(0.1595855, dtype=float32)]\n",
      "For epoch  81 loss is  [array(0.15773875, dtype=float32)]\n",
      "For epoch  82 loss is  [array(0.15593065, dtype=float32)]\n",
      "For epoch  83 loss is  [array(0.15416268, dtype=float32)]\n",
      "For epoch  84 loss is  [array(0.15243539, dtype=float32)]\n",
      "For epoch  85 loss is  [array(0.15074463, dtype=float32)]\n",
      "For epoch  86 loss is  [array(0.1490895, dtype=float32)]\n",
      "For epoch  87 loss is  [array(0.14746627, dtype=float32)]\n",
      "For epoch  88 loss is  [array(0.1458727, dtype=float32)]\n",
      "For epoch  89 loss is  [array(0.14430915, dtype=float32)]\n",
      "For epoch  90 loss is  [array(0.14277405, dtype=float32)]\n",
      "For epoch  91 loss is  [array(0.14126495, dtype=float32)]\n",
      "For epoch  92 loss is  [array(0.13978106, dtype=float32)]\n",
      "For epoch  93 loss is  [array(0.13832156, dtype=float32)]\n",
      "For epoch  94 loss is  [array(0.13688721, dtype=float32)]\n",
      "For epoch  95 loss is  [array(0.13547552, dtype=float32)]\n",
      "For epoch  96 loss is  [array(0.13408713, dtype=float32)]\n",
      "For epoch  97 loss is  [array(0.13272297, dtype=float32)]\n",
      "For epoch  98 loss is  [array(0.13138291, dtype=float32)]\n",
      "For epoch  99 loss is  [array(0.13006648, dtype=float32)]\n",
      "For epoch  100 loss is  [array(0.12877114, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for e in range(num_epochs):\n",
    "    e_losses = train_epoch(model,optimizer ,criteria,x_train.shape[0])\n",
    "    print(\"For epoch \",e+1,\"loss is \",e_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numpy = test_data.values\n",
    "output1 = test_numpy[:,-1]\n",
    "output1=output1.astype('float64')\n",
    "test_sub = test_data.drop(['date','Occupancy'],axis=1)\n",
    "x_test = changeToTensor(test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test>0.5]=1\n",
    "test[test<=0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is  93.53978671041837\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(test.shape[0]):\n",
    "    if(output1[i]==test[i]):\n",
    "        count = count+1\n",
    "print(\"Testing accuracy is \", (count/test.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a model with pytorch#####\n",
    "class Net_5(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super(Net_5, self).__init__()\n",
    "        self.fc1 = nn.Linear(size,5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Net_5(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_5(\n",
       "  (fc1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss across epoch is  [0.6266815798675142, 0.5735864021429201, 0.5738175471381444, 0.5740117686914235, 0.5743453638946138, 0.5742437408828154, 0.5742889829888577, 0.5743204904038731, 0.574344108199201, 0.574362125883742, 0.5743764300898808, 0.5743879192486042, 0.5743973475040459, 0.5744051200951018, 0.574411821801488, 0.5744174019229121, 0.5744221888300849, 0.5744263024591818, 0.5744298542054688, 0.5744330857585116, 0.5744358439271043, 0.5744382993477147, 0.5744403199088283, 0.5744422085401488, 0.574443902366045, 0.5744453970252014, 0.5744467361307726, 0.5744478749792751, 0.5744490249127876, 0.5744499289771405, 0.5744510323899549, 0.5744517380144538, 0.5744524445475602, 0.5744531863346333, 0.5744536515416169, 0.5744541570907686, 0.5744545417951374, 0.5744550915026083, 0.5744556143153005, 0.574455944139783, 0.5744562067273187, 0.5744566036070266, 0.5744567460766653, 0.5744569823145866, 0.5744571474994101, 0.5744574535183791, 0.5744577057477904, 0.5744579225415136, 0.5744580197625044, 0.5744581974861098]  length of epoch is  50  for learning rate  0.1\n",
      "Loss across epoch is  [0.5385294313110957, 0.5344478580646399, 0.5321701546994652, 0.5308882446914185, 0.5301905545156177, 0.5298135465965038, 0.5296084006748548, 0.5294948600050879, 0.5294308186304278, 0.5293935092120636, 0.5293714562203826, 0.5293579695791732, 0.529349618932096, 0.5293443619114596, 0.5293411379907189, 0.5293390511012659, 0.5293378177575949, 0.5293371074083375, 0.52933664910677, 0.5293363963321942, 0.5293363052897337, 0.5293363192822875, 0.5293363514469891, 0.529336414504342, 0.5293364735638223, 0.529336645290619, 0.5293367850344356, 0.5293367881237007, 0.5293369242330876, 0.5293370230895716, 0.5293371181299047, 0.5293372091723652, 0.5293372120799088, 0.529337350551675, 0.52933743723282, 0.5293373954368801, 0.5293375086493608, 0.5293375250042939, 0.5293376015090361, 0.5293376305844726, 0.5293377270785774, 0.5293378606438637, 0.5293377147215169, 0.52933784174483, 0.5293377977682323, 0.5293378015843834, 0.5293377935886383, 0.5293379465981227, 0.5293379929370996, 0.5293379544121463]  length of epoch is  50  for learning rate  0.01\n",
      "Loss across epoch is  [0.5247998324836173, 0.5247420701311856, 0.5247418887731505, 0.5247381245944558, 0.5247334521718141, 0.5247284213944179, 0.5247235256360798, 0.5247184514272504, 0.5247137926337195, 0.5247092684958039, 0.5247048470305233, 0.5247006230964893, 0.524696999388497, 0.5246932759154134, 0.52468987954099, 0.5246865892919098, 0.5246835154731099, 0.5246807233226009, 0.5246780367522705, 0.5246754653933572, 0.5246730025221662, 0.5246708776529242, 0.5246686919069872, 0.5246667849217973, 0.5246649037410573, 0.5246632760617791, 0.5246616127650913, 0.5246601722589354, 0.5246586354403961, 0.5246573708406309, 0.5246559815799318, 0.5246549770236015, 0.5246537642144575, 0.5246526580758211, 0.524651743653344, 0.5246507579960474, 0.5246500080315079, 0.5246491588470412, 0.5246483363756319, 0.5246476605534554, 0.5246469965431748, 0.524646392137539, 0.5246457833705879, 0.5246451689702708, 0.5246446049068032, 0.5246441664128769, 0.5246437328254304, 0.5246432625302454, 0.5246428656505375, 0.5246424729504237]  length of epoch is  50  for learning rate  0.001\n",
      "Loss across epoch is  [0.5241832896703627, 0.5241776065128606, 0.5241781789355162, 0.5241782563488658, 0.5241782759747854, 0.5241783497537055, 0.5241784315283705, 0.5241784437037096, 0.5241782232755567, 0.5241783226772052, 0.5241781182405425, 0.5241782287272011, 0.5241780010301892, 0.5241780831682973, 0.5241779755891823, 0.5241779646858936, 0.5241779559632627, 0.5241778809122923, 0.5241778356636443, 0.5241777122747607, 0.524177694102613, 0.5241776145086056, 0.5241775361866485, 0.5241775972450652, 0.5241774489603391, 0.5241774068009563, 0.5241774360581142, 0.5241772705098477, 0.5241773362930228, 0.5241771552984308, 0.5241771542081018, 0.5241770155546142, 0.5241769997448456, 0.5241769708511306, 0.5241769290551906, 0.5241768396482235, 0.5241768612730794, 0.524176736793867, 0.5241766233996648, 0.5241766975420278, 0.5241764289576832, 0.5241765801499529, 0.5241764462212237, 0.524176509823741, 0.5241763642648372, 0.5241763370066155, 0.5241762941203466, 0.5241761162150197, 0.5241762416028395, 0.5241760137241062]  length of epoch is  50  for learning rate  0.0001\n",
      "Loss across epoch is  [0.6644530917813138, 0.6059517260277416, 0.6061980759770405, 0.6060877091183168, 0.6060184735986518, 0.605975180728043, 0.6059459450132236, 0.6059248052628302, 0.6059091501846546, 0.6058969831139576, 0.6058872268058177, 0.6058794741132637, 0.6058730102348618, 0.6058677748845118, 0.6058632545173168, 0.6058595561672274, 0.6058561641541196, 0.6058533053118281, 0.6058508221786923, 0.6058486291638961, 0.6058467821922244, 0.6058450742374833, 0.6058436270984935, 0.605842381034319, 0.6058411769023756, 0.6058401504940376, 0.6058392422046603, 0.6058384787018706, 0.6058376461449193, 0.6058369364771174, 0.6058363631458544, 0.6058358074415747, 0.6058353176567612, 0.605834973976016, 0.6058344852815314, 0.6058339447964255, 0.6058338795129846, 0.6058336078848053, 0.6058333279428686, 0.6058331011090337, 0.6058329258386682, 0.6058325559445997, 0.6058325760702534, 0.605832165606865, 0.6058320299972121, 0.6058319085618344, 0.6058318609508072, 0.6058316365702123, 0.6058315002336735, 0.6058314511688744]  length of epoch is  50  for learning rate  0.2\n",
      "Loss across epoch is  [0.5473160623777203, 0.5387675591358324, 0.5359624775081147, 0.5350628088160259, 0.534763741602258, 0.534656253166315, 0.5346151341388865, 0.5345993570801688, 0.5345937862265401, 0.534592400781992, 0.5345927316968034, 0.5345936661086431, 0.5345947931452495, 0.5345958302297243, 0.5345969441823843, 0.5345979053072814, 0.5345986478212403, 0.534599363622142, 0.5345999811117242, 0.5346006209530482, 0.5346009376935843, 0.5346014112597559, 0.5346019533349247, 0.5346022664410311, 0.5346025948117419, 0.5346027968860254, 0.5346030725575075, 0.5346033387794727, 0.5346035642958269, 0.5346036727835493, 0.5346039197430378, 0.534604170336956, 0.5346042081350233, 0.5346043295249706, 0.5346044089372565, 0.534604557403704, 0.5346045595843617, 0.5346046995098998, 0.5346047362176384, 0.5346049172122304, 0.5346050338774193, 0.5346051180144635, 0.5346050758550807, 0.5346052277742362, 0.5346052175978335, 0.5346051648986049, 0.5346052541238505, 0.534605341713603, 0.5346053380791734, 0.5346053304468713]  length of epoch is  50  for learning rate  0.02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for learning_rate in [0.1,0.01,0.001,0.0001,0.2,0.02]:\n",
    "    num_epochs = 50\n",
    "    e_losses=[]\n",
    "    loss_across_epoch=[]\n",
    "    optimizer = torch.optim.Adam(model1.parameters(),lr=learning_rate)\n",
    "    for e in range(num_epochs):\n",
    "        e_losses = train_epoch(model1,optimizer ,criteria,100)\n",
    "        loss_to_plot = sum(e_losses)/len(e_losses)\n",
    "        loss_across_epoch.append(loss_to_plot)\n",
    "    plt.plot(loss_across_epoch)\n",
    "    print(\"Loss across epoch is \",loss_across_epoch,\" length of epoch is \",len(loss_across_epoch),\" for learning rate \",learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
