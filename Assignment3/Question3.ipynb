{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "from statistics import mode\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, opt, criterion, batch_size=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for index in range(0, x_train.shape[0], batch_size):\n",
    "        x_batch = x_train[index:index + batch_size, :]\n",
    "        label_batch = labels[index:index + batch_size, :]\n",
    "        opt.zero_grad()\n",
    "        prediction = model(x_batch)\n",
    "        loss = criterion(prediction,label_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    losses = sum(losses)/len(losses)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baselinePrediction(output1):\n",
    "    predictions_array=[]\n",
    "    for i in range(5):\n",
    "        test=np.random.randint(2, size=len(x_test))\n",
    "        test = list(test)\n",
    "        predictions_array.append(test)\n",
    "    predictions_array = np.array(predictions_array)\n",
    "    print(predictions_array.shape)\n",
    "    final_prediction =np.zeros(predictions_array.shape[1])\n",
    "    for i in range(predictions_array.shape[1]):\n",
    "        temp_list=predictions_array[:,i].tolist()\n",
    "        final_prediction[i]=mode(temp_list)\n",
    "    count=0\n",
    "    for i in range(output1.shape[0]):\n",
    "        if(output1[i]==final_prediction[i]):\n",
    "            count = count+1\n",
    "    print(\"Accuracy is \", (count/final_prediction.shape[0])*100)\n",
    "    return (count/final_prediction.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingAccuracy(model,x_test,output1):\n",
    "    predictions_array=[]\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_test)\n",
    "    test = np.asarray(predictions)\n",
    "    \n",
    "    test[test>0.5]=1\n",
    "    test[test<=0.5]=0\n",
    "\n",
    "    count=0\n",
    "    for i in range(test.shape[0]):\n",
    "        if(output1[i]==test[i]):\n",
    "            count = count+1\n",
    "    print(\"Accuracy is \", (count/test.shape[0])*100)\n",
    "    return (count/test.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the dataframe to numpy array to tensor array\n",
    "def changeToTensor(dataFrame):\n",
    "    dataFrame= (dataFrame-dataFrame.min())/(dataFrame.max()-dataFrame.min())\n",
    "    x = dataFrame.values\n",
    "    x= x.astype('float64')\n",
    "    x= torch.Tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(data):\n",
    "    data_numpy = data.values\n",
    "    labels = data_numpy[:,-1]\n",
    "    labels= labels.astype('float64')\n",
    "    #labels= torch.Tensor(labels)\n",
    "    data_numpy = data.drop(['date','Occupancy'],axis=1)\n",
    "    features = changeToTensor(data_numpy)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_2Layer(nn.Module):\n",
    "    def __init__(self,size,H_size,H2_size):\n",
    "        super(Net_2Layer, self).__init__()\n",
    "        self.fc1 = nn.Linear(size,H_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(H_size,H2_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(H2_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a model with pytorch#####\n",
    "class Net_H(nn.Module):\n",
    "    def __init__(self,size,H_size):\n",
    "        super(Net_H, self).__init__()\n",
    "        self.fc1 = nn.Linear(size,H_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(H_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a model with pytorch#####\n",
    "class Net_5(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super(Net_5, self).__init__()\n",
    "        self.fc1 = nn.Linear(size,5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria= nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.txt',sep=',')\n",
    "test_data = pd.read_csv('test.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, labels =preProcess(train_data)\n",
    "labels_accuracy = labels\n",
    "labels= torch.Tensor(labels)\n",
    "labels = labels.unsqueeze(1)\n",
    "print(x_train.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, output = preProcess(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a) printing the loss value across 100 epochs for batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_5(x_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_to_plot_batch=[]\n",
    "for e in range(num_epochs):\n",
    "    e_losses = train_epoch(model,optimizer ,criteria,x_train.shape[0])\n",
    "    loss_to_plot_batch.append(e_losses)\n",
    "    print(\"For epoch \",e+1,\"loss is \",e_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) Getting the baseline accuracy for the testing data from randomly generated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = baselinePrediction(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_error= 1-(baseline_accuracy/100)\n",
    "baseline_error_plot=[baseline_error]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) With the H=5,changed the learning rate and plotted training loss across every epoch. Also stopped when the error didnt decrease by much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "e_losses=[]\n",
    "loss_across_epoch=[]\n",
    "model = Net_5(x_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "for e in range(num_epochs):\n",
    "    e_losses = train_epoch(model,optimizer ,criteria,100)\n",
    "    scheduler.step()\n",
    "    if(len(loss_across_epoch)>1):\n",
    "        if(loss_across_epoch[-1]-e_losses)<0.0001:\n",
    "            num_epochs=e\n",
    "        else:\n",
    "            loss_across_epoch.append(e_losses)\n",
    "    else:\n",
    "        loss_across_epoch.append(e_losses)\n",
    "        \n",
    "print(\"Stopped running at \",len(loss_across_epoch),\" epochs rather than 100 epochs\")\n",
    "print(\"Accuracy for the testing data \")\n",
    "dummy=testingAccuracy(model,x_test,output)\n",
    "print(\"Accuracy for the training data\")\n",
    "dummy=testingAccuracy(model,x_train,labels_accuracy)#train_numpy[:,-1].astype('float64'))\n",
    "plt.title('Training loss across epoch vs number of epochs')\n",
    "plt.plot(loss_across_epoch)\n",
    "baseline_error_plot1 = baseline_error_plot[:len(loss_across_epoch)]\n",
    "plt.plot(baseline_error_plot1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) Plot of the training loss obtained by considering the entire training data as a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Batch Gradient descent method for training loss vs epoch')\n",
    "plt.plot(loss_to_plot_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) Varied the number of hidden layers(H) and created a model for each of it and plotted the training and testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis=[1,2,5,10,20]\n",
    "training_accuracy_every_H=[]\n",
    "testing_accuracy_every_H=[]\n",
    "num_epochs=100\n",
    "for H in [1,2,5,10,20]:\n",
    "    model = Net_H(x_train.shape[1],H)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    for e in range(num_epochs):\n",
    "        e_losses = train_epoch(model,optimizer ,criteria,x_train.shape[0])\n",
    "        scheduler.step()\n",
    "    print(\"Training accuracy for H \",H)\n",
    "    training_accuracy_every_H.append(testingAccuracy(model,x_train,labels_accuracy))#train_numpy[:,-1].astype('float64')))\n",
    "    print(\"Testing accuracy for H \",H)\n",
    "    testing_accuracy_every_H.append(testingAccuracy(model,x_test,output))\n",
    "plt.title('Training and Testing accuracy across H vs values of H')\n",
    "plt.plot(x_axis,training_accuracy_every_H)\n",
    "plt.plot(x_axis,testing_accuracy_every_H)\n",
    "plt.legend(['Training accuracy','Tesing accuracy'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (f) Changed the BCELoss to MSELoss and performed the training and testing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria= nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net_5(x_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_to_plot_batch=[]\n",
    "for e in range(num_epochs):\n",
    "    e_losses = train_epoch(model,optimizer ,criteria,x_train.shape[0])\n",
    "    loss_to_plot_batch.append(e_losses)\n",
    "plt.title('Batch Gradient descent method for training loss vs epoch')\n",
    "plt.plot(loss_to_plot_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters ():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Criterai is \",criteria)\n",
    "\n",
    "e_losses=[]\n",
    "loss_across_epoch=[]\n",
    "model = Net_5(x_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "for e in range(num_epochs):\n",
    "    e_losses = train_epoch(model,optimizer ,criteria,100)\n",
    "    scheduler.step()\n",
    "    if(len(loss_across_epoch)>1):\n",
    "        if(loss_across_epoch[-1]-e_losses)<0.0001:\n",
    "            num_epochs=e\n",
    "        else:\n",
    "            loss_across_epoch.append(e_losses)\n",
    "    else:\n",
    "        loss_across_epoch.append(e_losses)\n",
    "print(\"Stopped running at \",len(loss_across_epoch),\" epochs instead of 100\")\n",
    "\n",
    "\n",
    "\n",
    "dummy=testingAccuracy(model,x_test,output)\n",
    "print(\"Accuracy for the training data\")\n",
    "dummy=testingAccuracy(model,x_train,labels_accuracy)#train_numpy[:,-1].astype('float64'))\n",
    "plt.title('Training loss across epoch vs number of epochs')\n",
    "\n",
    "'''needed_index= 0\n",
    "min_change=0.001\n",
    "for i in range(1,len(loss_across_epoch)):\n",
    "    if(loss_across_epoch[i-1]-loss_across_epoch[i]< min_change):\n",
    "        print(\"Should stop at index \",i)\n",
    "        needed_index = i\n",
    "        break\n",
    "loss_across_epoch=loss_across_epoch[:needed_index]'''\n",
    "plt.plot(loss_across_epoch)    \n",
    "baseline_error_plot1 = baseline_error_plot[:len(loss_across_epoch)]\n",
    "plt.plot(baseline_error_plot1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs=100\n",
    "x_axis=[1,2,5,10,20]\n",
    "training_accuracy_every_H=[]\n",
    "testing_accuracy_every_H=[]\n",
    "for H in [1,2,5,10,20]:\n",
    "    model = Net_H(x_train.shape[1],H)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    for e in range(num_epochs):\n",
    "        e_losses = train_epoch(model,optimizer,criteria,x_train.shape[0])\n",
    "        scheduler.step()\n",
    "    training_accuracy_every_H.append(testingAccuracy(model,x_train,labels_accuracy))#train_numpy[:,-1].astype('float64')))\n",
    "    testing_accuracy_every_H.append(testingAccuracy(model,x_test,output))\n",
    "plt.title('Training and Testing accuracy across H vs values of H')\n",
    "plt.plot(x_axis,training_accuracy_every_H)\n",
    "plt.plot(x_axis,testing_accuracy_every_H)\n",
    "plt.legend(['Training accuracy','Tesing accuracy'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(g) Created a two layer network and obtained the test accuracy for every combination for H1 and H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=100\n",
    "criteria= nn.BCELoss()\n",
    "for H1 in [1,2,5,10,20]:\n",
    "    for H2 in [15,120,220,230]:\n",
    "        print()\n",
    "        print(\"H1 has \",H1,' neurons and H2 has',H2,' neurons')\n",
    "        model = Net_2Layer(x_train.shape[1],H1,H2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "        for e in range(num_epochs):\n",
    "            e_losses = train_epoch(model,optimizer ,criteria,x_train.shape[0])\n",
    "        print(\"Testing accuracy for the 2 layer for the model with H1 \",H1,\" and H2 \",H2)\n",
    "        testingAccuracy(model,x_test,output)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
